---
title: "Learn the overlaps: Advice for the Aspiring Data Scientist"
author: "Ilan"
cover: "/img/cover.jpg"
tags: ["datascience"]
date: 2018-10-01T21:29:29-04:00
draft: true
---

I often get asked by junior data professionals how they can either break into data science or elevate their game. I try to give advice that is both generic enough that can benefit anyone and specific enough that is actionable. Today I'll outline a generic framework for thinking about learning and provide a few concrete examples in support of it. These are tools that I still employ in my day to day learning and growing as a data professional.
 
<!--more-->

The general idea that I subscribe to is that there are various sources of information on any given topic, all of which have overlapping content. Imagine a Venn diagram. Each book you read or online course you take on a given subject will have some content that is shared and some that is specific to that author's perspective. My thesis is that when learning any given topic, you should strive to understand the most shared content first - that space of maximum overlap. Once you feel comfortable with the content in the middle, begin to move outwards to the edges. Roughly speaking, the density of the overlap is proportional to the value of the knowledge therein. This learning framework was alluded to by Tyler Cowen (for one) on the Ezra Klein podcast when he recommend reading [piles of books](https://www.vox.com/2018/5/21/17369920/ezra-klein-show-book-recommendations-tyler-cowen-shakespeare-amazon) instead of any individual book on a given subject. Paraphrasing, he was getting at the idea that there isn’t really a single definitive, true source on anything. There are many sources of knowledge that overlap in various areas. The best bang for your buck exists in mastering that overlapping space. With that as a frame, in this post I will provide a few pieces of advice on what every Data Scientist should be familiar with:

* Python / R / coding skills[^1]
* Basic stats
* Business applications

This post is skewed towards data scientists that perform analysis and model building for business intelligence as oppose to those who come from a computer science or ML background and spend a majority of their time productionalizing recommendation engines or tweaking deep learning models. I assume those folks are highly technical in ways that are relevant to their jobs and this advice may not map 1:1 with their responsibilities. 

<h3>Coding skills</h3>

I must underscore the importance of knowing how to code to be a successful data scientist. One simply cannot productionalize an Excel sheet and integrate into a larger infrastructure like you can with a piece of code. Excel (or any other GUI) is near impossible to debug or perform appropriate error checks (think about changing 1 random cell in a complex spreadsheet). And given all the tools and resources available online, you can learn how to build almost any model you want in any scripting language. Excel has its place in a data scientists toolkit but it should remain narrowly scoped and not used as a crutch.

Once you convince yourself that learning to code is critical, start with python[^2]. I recommend learning python from two distinct perspectives - Data Analysis and Software Engineering. Most aspiring DS's will likely download a [book](https://www.amazon.com/Python-Data-Analysis-Wrangling-IPython/dp/1449319793) about Data Analysis in Python or perhaps take [online](https://www.udemy.com/learning-python-for-data-analysis-and-visualization/?utm_source=adwords-learn&utm_medium=udemyads&utm_campaign=NEW-AW-PROS-TECH-US-DSA-1-EN-ENG_._ci__._sl_ENG_._vi_TECH_._sd_All_._la_EN_._&utm_content=deal4584&utm_term=_._ag_60674438611_._ad_267827191853_._de_c_._dm__._pl__._ti_dsa-304639795903_._li_9060351_._pd__._&gclid=EAIaIQobChMIjs2I3Yvo3QIVCIezCh0IQAd0EAAYAiAAEgIlf_D_BwE) [courses](https://www.coursera.org/learn/data-analysis-with-python). Luckily there is no shortage of good content. Don't stress much over which is the best one - remember to think in terms of piles not individual sources of knowledge. Pick one and work through the first few chapters which tend to cover basic concepts. Then download some datasets and hack on them using what you learned. Next, choose another book or course and repeat. Do this a few times and you'll notice where the overlap is. Then you can start getting into the later chapters where content tends to diverge. Just make sure you've homed in on the basics.

Learning to code like an analyst requires knowledge of python concepts like functions, list comprehension, string manipulation, basic web handling (to download datasets), data-centric libraries such as `numpy`, `pandas` and `sklearn` and plotting, using `matplotlib` or `seaborn` or whatever. You’ll actually do more plotting than many software engineers. This is all good stuff to know inside and out but its important to realize that most software engineers don't code this way. They don't generally use pandas for data manipulation, they rarely perform complex vector manipulations (`numpy`) and they don't plot (unless they are front-end). Finally, to perform Data analysis it's sufficient to have a small number of scripts that may or may not talk to each other. Software engineers on the other hand, write code that is sprawling and has much more structural complexity than a script that cleans, models and plots data. Since you'll be working closely with software engineers and there is a lot you can learn from them, it's useful to familiarize yourself with how they think about code.

After getting the basics of data analysis-centered python programming, move onto something more [generic](https://www.codecademy.com/learn/learn-python) like programming a [game](https://www.gamedesigning.org/learn/python/) or learning basic [backend](https://www.udacity.com/course/intro-to-backend--ud171) development. The point isn't to become a software engineer (don't worry a 3 week course in python won't get you there) but it's to identify how software engineers think about code in distinct ways from data scientists. Here you’ll use some of what you did before (lists, functions, generators, loops) and get exposure to other data structures like `dicts` and `sets`. You'll focus more on testing and error handling, [OO](https://python.swaroopch.com/oop.html) concepts like classes (don't be afraid of these) along with base modules like `itertools` and `collections`. All this stuff will serve to make you a better coder and model builder/deployer[^3].

It's also very helpful to become comfortable operating in a Unix shell - get away from windows and learn to love the command line. Start by customizing your terminal! Learn about your `.bashrc` and `.bash_profile` and customize it. Better yet, do it in [Vim](https://www.vim.org/)[^4]. This alone will elevate your Unix game quite a bit. You'll need it if you're going to get comfortable with `git`, environment variables or `.config`'s (if you want to connect to a database or external server) and navigating your project directory as you load data into your python script for analysis.

<h3>Basic stats</h3>

If you'll be building models and performing analysis, learn linear regression, hypothesis testing and logistic regression. These topics are foundational and complex enough to keep you busy for a while, especially if you don't have a strong Statistics background. 

To the point above about learning the overlap - understand that linear regression can be formulated in multiple ways, each with its own nuance. For example, formulating LR as a statistical problem involves thinking probabilistically, talking about residuals and making assumptions around independence, homoscedasticity and Normality. It also involves minimizing a log likelihood function and dovetails nicely into conversations around [Bayesian inference](https://en.wikipedia.org/wiki/Bayesian_linear_regression), correlations, hypothesis testing, etc... There is a lot that falls out of thinking of LR from a statistics perspective.

Formulating LR as a computer scientist would doesn't involve any of the above! It simply involves solving for a set of weights that minimizes the squared error between your estimate and the observation. No assumptions on Normality or independence or constant variance required. You end up with the same coefficients in both approaches, but now since you aren't making any assumptions on your data, you lose the ability to make inferences on the coefficients. This may or may not be useful to you, but, per the idea of learning the overlap, it's important to know that differences in these approaches exist.

A similar example exists when learning logistic regression. The statisticians have their approach to formulating this problem which involves assumptions on residuals and objects called logit and GLMs. Computer scientists use words like cross-entropy loss and sigmoid. Both approaches get you to the same set of weights which minimize the same error function, but one approach allows you to calculate odds-ratios and confidence intervals around your coefficients and the other is a basic building block of learning about neural networks (i.e., the later chapter in the book you're learning from). This stuff all builds on itself - don’t take short cuts. 

Once you've got a solid understanding of the various ways of formulating the same problem - prove it using data! Solve, from first principles, a LR or logistic regression problem from both a statistics or CS perspective. The best way to test your understanding is to make predictions *before* you run your model. Establish a hypothesis and see if your understanding and intuition line up with the results. Feel free to spend a lot of time in this phase, iterating, building and learning, before moving on to the "sexy", advanced material. 
 
<h3>Build some business intuition</h3>

Learn basic analytics. This includes cohort modeling, attribution and funnel analysis and how an A/B test works. Know what a marketing function does and why they (should) work closely with the Analytics team. Why should Product managers care about sample sizes? It's not rocket science so know this stuff because you'll be expected to. Also team up with the Finance function and learn how they think about the business. The leadership team ultimately cares most about the bottom line so understand how your models or business function supports the Financial model underlying the organization. It's very clarifying once you understand what matters most to the health of the company. I won't harp on this too much because most of it will come on the job. Don't rush it, but keep it in mind when you're building a dashboard or querying a database at the office. It's harder to learn this stuff on your own compared to Python basics or how Decision Trees work.

Speaking of practical business oriented advice, it'll be helpful to read a lot about why humans make bad (biased) decisions. Become a student of behavioral science and popular psychology (Daniel Kahneman and Amos Tversky, Richard Thaler, Dan Ariely, Nate Silver, even Malcolm Gladwell!). People make suboptimal decisions all the time. Data science is in large part a way to help people make better decisions by creating decision frameworks. Therefore it helps to learn where they are likely to screw up. Once you read a small pile of these books, you'll see a lot of overlapping themes and it'll be more readily apparent where you can add value with an unbiased, dispassionate analysis.

Data Scientists are expected to be among the more technically minded in the organization. You’ll be in more meetings with the business side than, say, software engineers will. As a result you need to learn their language. What are their pain points? When do they use words like "A caused B" even though causation is likely unknown? When they "project next year earnings at 10x this years'" ask how they got there. Maybe they used back of the envelope math which is sometimes appropriate but be sure to hold people accountable. You’ll also need to know how to explain things, so best to understand the subject matter from a few different perspectives. Consistent with the themes above, better to know a few useful things very well than a lot of things at the level of `from sklearn import...`.
 
<h3>Keep learning</h3>

Don't optimize locally. Get many perspectives (including on my advice!). Talk to many people in the industry. Go to Meetups, networking events, etc... Learn the vocabulary and start using it. Don't worry about being as efficient as possible in every endeavor all the time - you'll be working and learning for many years to come. Of course, it’s helpful to learn when you’ve wasted your time. It’ll happen so don't fret over it - instead use it as a learning experience. When you feel yourself veering away from the high density area of the Venn diagram, check if you're confident in your knowledge of the overlap and take small, incremental steps outwards.

[^1]: Not mentioning SQL or Excel because those are table stakes.
[^2]: Or R but probably python if you want to work in tech
[^3]: Also feel free to ditch Jupyter every now and then. It promotes bad coding practice and if you ever want your stuff to run in production, you'll likely need to convert your `.ipynb` to `.py`.
[^4]: Or emacs heaven help you.

