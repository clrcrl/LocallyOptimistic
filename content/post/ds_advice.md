---
title: "Learn the overlaps: Advice for the Aspiring Data Scientist"
author: "Ilan"
cover: "/img/cover.jpg"
tags: ["datascience"]
date: 2018-10-01T21:29:29-04:00
draft: true
---

I often get asked by junior data professionals who they can either break into data science or elevate their game. I try to give advice that is both generic enough that anyone can benefit and specific enough that is actionable for that person. Today I'll outline a generic framework for thinking about learning and provide a few concrete examples that support this way of thinking. These are tools that I still employ in my day to day learning and growing as a data professional.
 
<!--more-->

The learning framework that I trade in is echoed by Tyler Cowen’s sentiment in his podcast on the Ezra Klein show, when he recommend reading [piles of books](https://www.vox.com/2018/5/21/17369920/ezra-klein-show-book-recommendations-tyler-cowen-shakespeare-amazon) instead of any individual book on a given subject. Paraphrasing, he was getting at the idea that there isn’t really a single definitive, true source on anything. There are many sources of knowledge that overlap in various areas. The best bang for your buck exists in mastering that overlapping space. That is, you should strive to find the space of maximum overlap in the Venn diagram of the specific topic and learn that first. Once you feel comfortable with the content in the middle, begin to move outwards to the edges. In other words, the density of the overlap is roughly proportional to the value of the knowledge that lives there. 

With that as a frame, in this post I will provide some pieces of advice on what every Data Scientist should be familiar with:

* Python / R / coding skills
* Basic stats
* Business applications

This post is skewed towards data scientists that perform analysis and model building for business intelligence as oppose to those who come from a computer science or ML background and spend a majority of their time productionalizing recommendation engines or tweaking deep learning models. I assume those folks are highly technical in ways that are relevant to their jobs and this advice may not map 1:1 with their needs. 

<h3>Coding skills</h3>

Learn python or R (probably python if you want to work in tech - I'll be referring to python for the remainder of this post). It's also helpful to become comfortable operating in a Unix shell - get away from GUIs and learn to love the command line. Start by customizing your terminal! Learn about your `.bashrc` and `.bash_profile` and customize it. Better yet, do it in [Vim](https://www.vim.org/)[^1]. This alone will elevate your Unix game quite a bit. You'll need it if you're going to get comfortable with `git`, environment variables (if you want to connect to a database or external server) and navigating your project directory as you load data into your python script for analysis.

More importantly though, I recommend learning python from two distinct perspectives - Data Analysis and Software Engineering. Most aspiring DS's will likely download a [book](https://www.amazon.com/Python-Data-Analysis-Wrangling-IPython/dp/1449319793) about Data Analysis in Python or perhaps take [online](https://www.udemy.com/learning-python-for-data-analysis-and-visualization/?utm_source=adwords-learn&utm_medium=udemyads&utm_campaign=NEW-AW-PROS-TECH-US-DSA-1-EN-ENG_._ci__._sl_ENG_._vi_TECH_._sd_All_._la_EN_._&utm_content=deal4584&utm_term=_._ag_60674438611_._ad_267827191853_._de_c_._dm__._pl__._ti_dsa-304639795903_._li_9060351_._pd__._&gclid=EAIaIQobChMIjs2I3Yvo3QIVCIezCh0IQAd0EAAYAiAAEgIlf_D_BwE) [courses](https://www.coursera.org/learn/data-analysis-with-python). Luckily there is no shortage of good content. Don't stress much over which is the best one - remember we want piles not individual sources of knowledge. Pick one and work through it. Then download some datasets and hack on them. Solve problems. Get a bunch of data, clean it, model it and visualize it. Then choose another book and breeze through it filling in the gaps that the first one left. Note that this type of learning will skew heavily towards single purpose, modular programs. Given some input, produce an output. It’s easy to start here because you don’t have to think about the greater infrastructure required to support such a program in production. The amount of overhead required to know to produce a useful script that makes predictions is much lower than for, say, programming a game or application with a front end and a backend. This makes it a lower barrier to learn something effective - quick wins are great for beginners!

Data analysis requires knowledge of python concepts like functions, list comprehension, string manipulation, basic web handling (to download datasets), data-centric libraries such as `numpy`, `pandas` and `sklearn` and plotting, using `matplotlib` or `seaborn` or whatever. You’ll actually do more plotting than many software engineers. This is all good stuff to know inside and out. But its important to realize that most software engineers don't code this way. That is, they don't generally use pandas for data manipulation, they rarely perform complex vector manipulations (`numpy`) and they don't plot (unless they are front-end). Since you'll be working closely (hopefully) with software engineers and there is a lot you can learn from them, it's useful to familiarize yourself with how they solve problems.

After getting the basics of data analysis-centered Python programming, move onto something more [generic](https://www.codecademy.com/learn/learn-python) like programming a [game](https://www.gamedesigning.org/learn/python/) or learning basic [backend](https://www.udacity.com/course/intro-to-backend--ud171) development. The point isn't to become a software engineer (don't worry a 3 week course in Python won't get you there) but it's to identify how software engineers think about code in distinct ways from data scientists. Here you’ll use some of what you did before (lists, functions, generators, loops) and get exposure to other data structures like `dicts` and `sets`, you'll focus more on testing and erroor handling, OO concepts like classes (don't be afraid of these) along with base modules like `itertools` and `collections`. All this stuff will serve to make you a better coder and model builder/deployer[^2].

<h3>Basic stats</h3>

Learn linear regression, hypothesis testing and logistic regression. These topics are foundational and complex enough to keep you busy for a while, especially if you don't have a strong Statistics background. 

To the point above about learning the overlap - understand that linear regression can be formulated in multiple ways, each with its own nuance. For example, formulating LR as a statistical problem involves thinking probabilistically, talking about residuals and making assumptions around independence, homoscedasticity and Normality. It also involves minimizing a log likelihood function and dovetails nicely into conversations around [Bayesian inference](https://en.wikipedia.org/wiki/Bayesian_linear_regression), correlations, hypothesis testing, etc... There is a lot that falls out of thinking of LR from a statistics perspective.

Formulating LR as a computer scientist would doesn't involve any of the above! It simply involves solving for a set of weights that minimizes the squared error between your estimate and the observation. No assumptions on Normality or independence or constant variance required. You end up with the same coefficients in both approaches, but now since you aren't making any assumptions on your data, you lose the ability to make inferences on the coefficients. This may or may not be useful to you, but, per the idea of learning the overlap, it's important to know that differences in these approaches exist.

A similar example exists when learning logistic regression. The statisticians have their approach to formulating this problem which involves assumptions on residuals and objects called logit and GLMs. Computer scientists use words like cross-entropy loss and sigmoid. Both approaches get you to the same set of weights which minimize the same error function, but one approach allows you to calculate odds-ratios and confidence intervals around your coefficients and the other is a basic building block of learning about neural networks. Learn about how other fields formulate the same problem, in part to know how it works, and in part to be able to read the literature as you learn more. Data Science is built upon various fields each with their own problem formulation and terminology. Don't get confused.

This stuff all builds on itself - don’t take short cuts. Understand what you’re doing and why. Learn the basics, but learn them well.

<h3>Build some business intuition</h3>

Learn basic analytics. This includes cohort modeling, attribution and funnel analysis and how an A/B testing works. Know what a marketing function does and why they (should) work closely with the Analytics team. Why should Product managers care about sample sizes? It's not rocket science so know this stuff because you'll be expected to. I won't harp on this too much because most of it will come on the job. Don't rush it, but keep it in mind when you're building a dashboard or querying a database at the office. It's harder to learn this stuff on your own compared to Python basics or how Decision Trees work.

Speaking of practical business oriented advice, it'll be helpful to read a lot about why humans make bad (biased) decisions. Become a student of behavioral science and popular psychology (Daniel Kahneman and Amos Tversky, Richard Thaler, Dan Ariely, Nate Silver, even Malcolm Gladwell!). Humans make bad decisions all the time. Data science is in large part a way to help people make better decisions. Therefore it helps to learn where they are likely to screw up. Once you read a small pile of these books, you'll see a lot of overlapping themes and it'll be more readily apparent. I've used the concept of [nudges](https://www.amazon.com/Nudge-Improving-Decisions-Health-Happiness/dp/014311526X) at work to get things across the line countless times.

Data Scientists are expected to be among the more technically minded in the organization. You’ll be in more meetings with the business side than, say, software engineers will. As a result you need to learn their language. What are their pain points? When do they use words like "A caused B" even though causation is likely unknown? When they "project next year earnings at 10x this years'" ask how they got there. Maybe they used back of the envelope math, which is sometimes appropriate but be sure to hold people accountable. You’ll also need to know how to explain things. So know your stuff really well. Consistent with the themes above, better to know a few useful things very well than a lot of things at the level of `from sklearn import...`.
 
<h3>Keep learning</h3>

Don't optimize locally. Get many perspectives (including on my advice!). Talk to many people in the industry. Go to Meetups, networking events, etc... Learn the vocabulary and start using it. Don't worry about being as efficient as possible in every endeavor all the time - you'll be working and learning for many years to come. Of course, it’s helpful to learn when you’ve wasted your time. It’ll happen. But learn from it. Don’t let the perfect be the enemy of the good. When you feel yourself veering away from the high density area of the Venn diagram, check if you're confident in your knowledge of the overlap and take small, incremental steps outwards.

[^1]: Or emacs heaven help you.
[^2]: Also feel free to ditch Jupyter every now and then. It promotes bad coding practice and if you ever want your stuff to run in production, you'll likely need to convert your `.ipynb` to `.py`.
